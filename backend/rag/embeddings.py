"""
Embeddings - Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector embeddings
"""
from config.config import config
from typing import List
from sentence_transformers import SentenceTransformer
import numpy as np
import sys
from pathlib import Path

# Th√™m path ƒë·ªÉ import config
sys.path.append(str(Path(__file__).parent.parent.parent))


class EmbeddingModel:
    """Class qu·∫£n l√Ω embedding model"""

    def __init__(self, model_name: str = None, use_vietnamese: bool = True):
        """
        Kh·ªüi t·∫°o embedding model

        Args:
            model_name: T√™n model (n·∫øu None s·∫Ω d√πng t·ª´ config)
            use_vietnamese: True = d√πng model ti·∫øng Vi·ªát, False = model ti·∫øng Anh
        """
        if model_name:
            self.model_name = model_name
        else:
            # Ch·ªçn model d·ª±a tr√™n ng√¥n ng·ªØ
            if use_vietnamese:
                self.model_name = config.EMBEDDING_MODEL_VI
                print(f"üáªüá≥ S·ª≠ d·ª•ng model ti·∫øng Vi·ªát: {self.model_name}")
            else:
                self.model_name = config.EMBEDDING_MODEL
                print(f"üåç S·ª≠ d·ª•ng model ƒëa ng√¥n ng·ªØ: {self.model_name}")

        print(f"‚è≥ ƒêang t·∫£i model embedding...")
        try:
            self.model = SentenceTransformer(self.model_name)
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            print(f"‚úÖ Model ƒë√£ s·∫µn s√†ng! (Dimension: {self.embedding_dim})")
        except Exception as e:
            print(f"‚ùå L·ªói t·∫£i model: {e}")
            print(f"üí° Th·ª≠ d√πng model backup...")
            # Fallback v·ªÅ model nh·∫π h∆°n
            self.model_name = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
            self.model = SentenceTransformer(self.model_name)
            self.embedding_dim = self.model.get_sentence_embedding_dimension()
            print(f"‚úÖ ƒê√£ t·∫£i model backup: {self.model_name}")

    def encode_text(self, text: str) -> np.ndarray:
        """
        Chuy·ªÉn m·ªôt vƒÉn b·∫£n th√†nh vector

        Args:
            text: VƒÉn b·∫£n c·∫ßn encode

        Returns:
            numpy.ndarray: Vector embedding
        """
        if not text or not text.strip():
            return np.zeros(self.embedding_dim)

        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding

    def encode_batch(
        self,
        texts: List[str],
        batch_size: int = 32,
        show_progress: bool = True
    ) -> np.ndarray:
        """
        Chuy·ªÉn nhi·ªÅu vƒÉn b·∫£n th√†nh vectors (nhanh h∆°n encode t·ª´ng c√°i)

        Args:
            texts: Danh s√°ch vƒÉn b·∫£n
            batch_size: S·ªë vƒÉn b·∫£n x·ª≠ l√Ω c√πng l√∫c
            show_progress: Hi·ªÉn th·ªã progress bar

        Returns:
            numpy.ndarray: Ma tr·∫≠n embeddings (n_texts, embedding_dim)
        """
        if not texts:
            return np.array([])

        # Lo·∫°i b·ªè text r·ªóng
        valid_texts = [
            text if text and text.strip() else " " for text in texts]

        embeddings = self.model.encode(
            valid_texts,
            batch_size=batch_size,
            show_progress_bar=show_progress,
            convert_to_numpy=True
        )

        return embeddings

    def encode_documents(self, documents: List[dict]) -> List[dict]:
        """
        Encode danh s√°ch documents (v·ªõi metadata)

        Args:
            documents: List[{'content': str, 'metadata': dict}]

        Returns:
            List[{'content': str, 'metadata': dict, 'embedding': np.ndarray}]
        """
        if not documents:
            return []

        # L·∫•y content
        contents = [doc.get('content', '') for doc in documents]

        # Encode batch
        print(f"‚è≥ ƒêang encode {len(contents)} documents...")
        embeddings = self.encode_batch(contents, show_progress=True)

        # G·∫Øn embedding v√†o documents
        for i, doc in enumerate(documents):
            doc['embedding'] = embeddings[i]

        print(f"‚úÖ ƒê√£ encode {len(documents)} documents!")
        return documents

    def similarity(self, text1: str, text2: str) -> float:
        """
        T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa 2 vƒÉn b·∫£n (cosine similarity)

        Args:
            text1: VƒÉn b·∫£n th·ª© nh·∫•t
            text2: VƒÉn b·∫£n th·ª© hai

        Returns:
            float: ƒêi·ªÉm t∆∞∆°ng ƒë·ªìng (0-1)
        """
        emb1 = self.encode_text(text1)
        emb2 = self.encode_text(text2)

        # Cosine similarity
        similarity = np.dot(emb1, emb2) / \
            (np.linalg.norm(emb1) * np.linalg.norm(emb2))
        return float(similarity)

    def get_model_info(self) -> dict:
        """L·∫•y th√¥ng tin v·ªÅ model"""
        return {
            'model_name': self.model_name,
            'embedding_dimension': self.embedding_dim,
            'max_seq_length': self.model.max_seq_length
        }


def demo_embeddings():
    """Demo ch·ª©c nƒÉng embedding"""
    print("=" * 60)
    print("DEMO - TEXT EMBEDDINGS")
    print("=" * 60)

    # T·∫°o embedding model
    embedder = EmbeddingModel(use_vietnamese=True)

    # Hi·ªÉn th·ªã info
    info = embedder.get_model_info()
    print(f"\nüìä Th√¥ng tin model:")
    print(f"  - Model: {info['model_name']}")
    print(f"  - Dimension: {info['embedding_dimension']}")
    print(f"  - Max sequence length: {info['max_seq_length']}")

    # Test encode m·ªôt vƒÉn b·∫£n
    print("\n" + "=" * 60)
    print("TEST 1 - ENCODE ƒê∆†N")
    print("=" * 60)

    text = "ƒêau ƒë·∫ßu c√≥ th·ªÉ do cƒÉng th·∫≥ng ho·∫∑c m·∫•t ng·ªß"
    embedding = embedder.encode_text(text)

    print(f"VƒÉn b·∫£n: '{text}'")
    print(f"Embedding shape: {embedding.shape}")
    print(f"Embedding (10 s·ªë ƒë·∫ßu): {embedding[:10]}")

    # Test encode nhi·ªÅu vƒÉn b·∫£n
    print("\n" + "=" * 60)
    print("TEST 2 - ENCODE BATCH")
    print("=" * 60)

    texts = [
        "Tri·ªáu ch·ª©ng c·∫£m c√∫m bao g·ªìm s·ªët v√† ƒëau ƒë·∫ßu",
        "Ngh·ªâ ng∆°i v√† u·ªëng nhi·ªÅu n∆∞·ªõc khi b·ªã c·∫£m",
        "Vitamin C gi√∫p tƒÉng c∆∞·ªùng h·ªá mi·ªÖn d·ªãch",
        "ƒêau b·ª•ng c√≥ th·ªÉ do ƒÉn u·ªëng kh√¥ng h·ª£p l√Ω"
    ]

    embeddings = embedder.encode_batch(texts, show_progress=False)
    print(f"S·ªë vƒÉn b·∫£n: {len(texts)}")
    print(f"Embeddings shape: {embeddings.shape}")

    # Test similarity
    print("\n" + "=" * 60)
    print("TEST 3 - ƒê·ªò T∆Ø∆†NG ƒê·ªíNG")
    print("=" * 60)

    query = "L√†m sao ƒë·ªÉ ƒëi·ªÅu tr·ªã c·∫£m c√∫m?"

    print(f"C√¢u h·ªèi: '{query}'")
    print(f"\nƒê·ªô t∆∞∆°ng ƒë·ªìng v·ªõi c√°c c√¢u kh√°c:")

    for text in texts:
        sim = embedder.similarity(query, text)
        print(f"  - [{sim:.3f}] {text}")

    # Test encode documents
    print("\n" + "=" * 60)
    print("TEST 4 - ENCODE DOCUMENTS")
    print("=" * 60)

    documents = [
        {
            'content': 'C·∫£m c√∫m l√† b·ªánh do virus g√¢y ra',
            'metadata': {'source': 'doc1.pdf', 'page': 1}
        },
        {
            'content': 'S·ªët cao l√† tri·ªáu ch·ª©ng c·ªßa c·∫£m c√∫m',
            'metadata': {'source': 'doc2.pdf', 'page': 5}
        }
    ]

    docs_with_embeddings = embedder.encode_documents(documents)

    print(f"\nK·∫øt qu·∫£:")
    for i, doc in enumerate(docs_with_embeddings, 1):
        print(f"Document {i}:")
        print(f"  Content: {doc['content']}")
        print(f"  Metadata: {doc['metadata']}")
        print(f"  Embedding shape: {doc['embedding'].shape}")

    print("\n‚úÖ Demo ho√†n t·∫•t!")


if __name__ == "__main__":
    demo_embeddings()
