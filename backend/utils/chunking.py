"""
Text Chunking - Chia nhá» vÄƒn báº£n thÃ nh cÃ¡c Ä‘oáº¡n Ä‘á»ƒ xá»­ lÃ½ RAG
"""
from config.config import config
from typing import List, Dict
from langchain.text_splitter import RecursiveCharacterTextSplitter
import sys
from pathlib import Path

# ThÃªm path Ä‘á»ƒ import config
sys.path.append(str(Path(__file__).parent.parent.parent))


class DocumentChunker:
    """Class Ä‘á»ƒ chia nhá» documents thÃ nh chunks"""

    def __init__(
        self,
        chunk_size: int = None,
        chunk_overlap: int = None,
        separators: List[str] = None
    ):
        """
        Khá»Ÿi táº¡o chunker

        Args:
            chunk_size: KÃ­ch thÆ°á»›c tá»‘i Ä‘a cá»§a má»—i chunk (characters)
            chunk_overlap: Sá»‘ kÃ½ tá»± chá»“ng láº¥p giá»¯a cÃ¡c chunk
            separators: Danh sÃ¡ch kÃ½ tá»± phÃ¢n tÃ¡ch Æ°u tiÃªn
        """
        self.chunk_size = chunk_size or config.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or config.CHUNK_OVERLAP

        # Separators cho tiáº¿ng Viá»‡t
        if separators is None:
            separators = [
                "\n\n",  # Paragraph
                "\n",    # New line
                ". ",    # Sentence end
                "! ",    # Exclamation
                "? ",    # Question
                "; ",    # Semicolon
                ", ",    # Comma
                " ",     # Space
                ""       # Character
            ]

        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=separators,
            length_function=len,
        )

    def chunk_text(self, text: str, metadata: Dict = None) -> List[Dict]:
        """
        Chia má»™t vÄƒn báº£n thÃ nh nhiá»u chunks

        Args:
            text: VÄƒn báº£n cáº§n chia
            metadata: Metadata gáº¯n vá»›i vÄƒn báº£n

        Returns:
            List[Dict]: Danh sÃ¡ch chunks vá»›i content vÃ  metadata
        """
        if not text or not text.strip():
            return []

        # Chia vÄƒn báº£n
        chunks = self.text_splitter.split_text(text)

        # Táº¡o list documents
        documents = []
        for i, chunk in enumerate(chunks):
            doc_metadata = metadata.copy() if metadata else {}
            doc_metadata['chunk_index'] = i
            doc_metadata['total_chunks'] = len(chunks)

            documents.append({
                'content': chunk,
                'metadata': doc_metadata
            })

        return documents

    def chunk_documents(self, documents: List[Dict]) -> List[Dict]:
        """
        Chia nhiá»u documents thÃ nh chunks

        Args:
            documents: Danh sÃ¡ch documents vá»›i format:
                      [{'content': str, 'metadata': dict}, ...]

        Returns:
            List[Dict]: Danh sÃ¡ch chunks
        """
        all_chunks = []

        for doc in documents:
            content = doc.get('content', '')
            metadata = doc.get('metadata', {})

            chunks = self.chunk_text(content, metadata)
            all_chunks.extend(chunks)

        return all_chunks

    def chunk_by_sentences(
        self,
        text: str,
        sentences_per_chunk: int = 5,
        metadata: Dict = None
    ) -> List[Dict]:
        """
        Chia vÄƒn báº£n theo sá»‘ cÃ¢u

        Args:
            text: VÄƒn báº£n cáº§n chia
            sentences_per_chunk: Sá»‘ cÃ¢u má»—i chunk
            metadata: Metadata

        Returns:
            List[Dict]: Danh sÃ¡ch chunks
        """
        # TÃ¡ch cÃ¢u (Ä‘Æ¡n giáº£n)
        sentences = []
        for sep in ['. ', '! ', '? ']:
            if sep in text:
                text = text.replace(sep, sep + '<<<SPLIT>>>')

        sentences = text.split('<<<SPLIT>>>')
        sentences = [s.strip() for s in sentences if s.strip()]

        # NhÃ³m cÃ¢u thÃ nh chunks
        chunks = []
        for i in range(0, len(sentences), sentences_per_chunk):
            chunk_sentences = sentences[i:i + sentences_per_chunk]
            chunk_text = ' '.join(chunk_sentences)

            doc_metadata = metadata.copy() if metadata else {}
            doc_metadata['chunk_index'] = len(chunks)
            doc_metadata['sentence_start'] = i
            doc_metadata['sentence_end'] = min(
                i + sentences_per_chunk, len(sentences))

            chunks.append({
                'content': chunk_text,
                'metadata': doc_metadata
            })

        return chunks


def demo_chunking():
    """Demo chá»©c nÄƒng chunking"""
    print("=" * 60)
    print("DEMO - DOCUMENT CHUNKING")
    print("=" * 60)

    # Táº¡o chunker
    chunker = DocumentChunker(chunk_size=200, chunk_overlap=50)

    # Text máº«u
    sample_text = """
    Cáº£m cÃºm lÃ  bá»‡nh nhiá»…m virus cÃºm, lÃ¢y lan qua Ä‘Æ°á»ng hÃ´ háº¥p. 
    Bá»‡nh thÆ°á»ng xuáº¥t hiá»‡n vÃ o mÃ¹a Ä‘Ã´ng khi thá»i tiáº¿t láº¡nh.
    
    Triá»‡u chá»©ng phá»• biáº¿n bao gá»“m:
    - Sá»‘t cao Ä‘á»™t ngá»™t (38-40Â°C)
    - Äau Ä‘áº§u vÃ  Ä‘au cÆ¡
    - Äau há»ng vÃ  ho
    - Má»‡t má»i toÃ n thÃ¢n
    - Cháº£y nÆ°á»›c mÅ©i hoáº·c ngháº¹t mÅ©i
    
    CÃ¡ch phÃ²ng ngá»«a hiá»‡u quáº£:
    1. Rá»­a tay thÆ°á»ng xuyÃªn báº±ng xÃ  phÃ²ng
    2. Äeo kháº©u trang khi ra nÆ¡i Ä‘Ã´ng ngÆ°á»i
    3. TÄƒng cÆ°á»ng há»‡ miá»…n dá»‹ch qua Äƒn uá»‘ng vÃ  váº­n Ä‘á»™ng
    4. TiÃªm vaccine phÃ²ng cÃºm hÃ ng nÄƒm
    5. TrÃ¡nh tiáº¿p xÃºc gáº§n vá»›i ngÆ°á»i bá»‡nh
    
    Khi nÃ o cáº§n Ä‘i khÃ¡m bÃ¡c sÄ©:
    - Sá»‘t cao trÃªn 39Â°C kÃ©o dÃ i >3 ngÃ y
    - KhÃ³ thá»Ÿ, Ä‘au ngá»±c
    - Triá»‡u chá»©ng náº·ng hÆ¡n sau vÃ i ngÃ y Ä‘iá»u trá»‹
    - NgÆ°á»i cao tuá»•i, tráº» em nhá», phá»¥ ná»¯ mang thai
    """

    metadata = {
        'source': 'health_guide.pdf',
        'topic': 'Cáº£m cÃºm',
        'page': 15
    }

    # Chunk vÄƒn báº£n
    chunks = chunker.chunk_text(sample_text, metadata)

    print(f"\nğŸ“Š Káº¿t quáº£:")
    print(f"  - Chunk size: {chunker.chunk_size}")
    print(f"  - Chunk overlap: {chunker.chunk_overlap}")
    print(f"  - Sá»‘ chunks táº¡o ra: {len(chunks)}")

    print(f"\nğŸ“„ Chi tiáº¿t cÃ¡c chunks:\n")
    for i, chunk in enumerate(chunks, 1):
        print(f"Chunk {i}/{len(chunks)}:")
        print(f"  Äá»™ dÃ i: {len(chunk['content'])} kÃ½ tá»±")
        print(f"  Metadata: {chunk['metadata']}")
        print(f"  Ná»™i dung:")
        print(f"  ---")
        print(f"  {chunk['content'][:150]}...")
        print()

    # Demo chunk theo cÃ¢u
    print("\n" + "=" * 60)
    print("DEMO - CHUNKING THEO CÃ‚U")
    print("=" * 60)

    short_text = "Äau Ä‘áº§u cÃ³ thá»ƒ do nhiá»u nguyÃªn nhÃ¢n. CÄƒng tháº³ng lÃ  nguyÃªn nhÃ¢n phá»• biáº¿n. Máº¥t ngá»§ cÅ©ng gÃ¢y Ä‘au Ä‘áº§u. Uá»‘ng Ã­t nÆ°á»›c khiáº¿n Ä‘au Ä‘áº§u tÄƒng. Cáº§n nghá»‰ ngÆ¡i vÃ  uá»‘ng Ä‘á»§ nÆ°á»›c."

    sentence_chunks = chunker.chunk_by_sentences(
        short_text,
        sentences_per_chunk=2,
        metadata={'source': 'tips.txt'}
    )

    print(f"\nğŸ“Š Káº¿t quáº£:")
    print(f"  Sá»‘ chunks: {len(sentence_chunks)}")

    for i, chunk in enumerate(sentence_chunks, 1):
        print(f"\nChunk {i}:")
        print(f"  {chunk['content']}")
        print(
            f"  CÃ¢u tá»« {chunk['metadata']['sentence_start']} Ä‘áº¿n {chunk['metadata']['sentence_end']}")


if __name__ == "__main__":
    demo_chunking()
